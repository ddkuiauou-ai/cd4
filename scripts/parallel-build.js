#!/usr/bin/env node
/**
 * CD3 Project - Parallel Build Script
 *
 * ì¢…ëª©ì„ ì²­í¬ë³„ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ë¹Œë“œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */

const { spawn } = require("child_process");
const fs = require("fs");
const path = require("path");

const CHUNK_SIZE = parseInt(process.env.CHUNK_SIZE) || 400; // 2ì½”ì–´ 8GB í™˜ê²½ìš© ì†ë„ ìµœì í™” í•œ ì²­í¬ë‹¹ ì¢…ëª© ìˆ˜
const MAX_PARALLEL = parseInt(process.env.MAX_PARALLEL) || 2; // 2ì½”ì–´ í™œìš©í•˜ì—¬ ì†ë„ í–¥ìƒ

console.log("ğŸš€ Starting parallel SSG build...");
console.log(`ğŸ“Š Configuration:`);
console.log(`   - Chunk size: ${CHUNK_SIZE} securities per chunk`);
console.log(`   - Max parallel processes: ${MAX_PARALLEL}`);

/**
 * ì¢…ëª© ì²­í¬ë³„ë¡œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ë¹Œë“œ ì‹¤í–‰
 */
async function runBuildChunk(chunkIndex, totalChunks) {
  return new Promise((resolve, reject) => {
    const chunkId = `chunk-${chunkIndex}`;
    const env = {
      ...process.env,
      BUILD_CHUNK_INDEX: chunkIndex.toString(),
      BUILD_CHUNK_TOTAL: totalChunks.toString(),
      BUILD_CHUNK_SIZE: CHUNK_SIZE.toString(),
      // ğŸ¯ ê° ì²­í¬ë³„ë¡œ ë…ë¦½ì ì¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
      NEXT_BUILD_DIR: `.next-${chunkId}`,
      BUILD_OUTPUT_DIR: `out-${chunkId}`,
      NEXT_CACHE_DIR: `.next-cache-${chunkId}`,
      BUILD_ID: `${chunkId}-${Date.now()}`,
      // í¬íŠ¸ë„ ë‹¤ë¥´ê²Œ ì„¤ì • (dev ëª¨ë“œì—ì„œ ì¶©ëŒ ë°©ì§€)
      PORT: (3000 + chunkIndex).toString(),
    };

    console.log(
      `ğŸ”¨ Starting build chunk ${chunkIndex + 1}/${totalChunks} (${chunkId})...`
    );
    console.log(`ğŸ“ Output directory: out-${chunkId}`);

    const buildProcess = spawn("pnpm", ["run", "build"], {
      env,
      stdio: ["inherit", "pipe", "pipe"],
      cwd: process.cwd(),
    });

    let stdout = "";
    let stderr = "";

    buildProcess.stdout.on("data", (data) => {
      stdout += data.toString();
      // ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥ (ì²­í¬ ë²ˆí˜¸ í¬í•¨)
      process.stdout.write(`[${chunkId}] ${data}`);
    });

    buildProcess.stderr.on("data", (data) => {
      stderr += data.toString();
      process.stderr.write(`[${chunkId}] ${data}`);
    });

    buildProcess.on("close", (code) => {
      if (code === 0) {
        console.log(
          `âœ… Chunk ${
            chunkIndex + 1
          }/${totalChunks} (${chunkId}) completed successfully`
        );
        resolve({ chunkIndex, chunkId, stdout, stderr });
      } else {
        console.error(
          `âŒ Chunk ${
            chunkIndex + 1
          }/${totalChunks} (${chunkId}) failed with code ${code}`
        );
        reject(new Error(`Build chunk ${chunkIndex + 1} failed`));
      }
    });
  });
}

/**
 * ì²­í¬ë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì‹¤í–‰
 */
async function runParallelBuild() {
  try {
    // ì´ ì¢…ëª© ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
    const estimatedSecurities = parseInt(process.env.TOTAL_SECURITIES) || 2500;
    const totalChunks = Math.ceil(estimatedSecurities / CHUNK_SIZE);

    console.log(
      `ğŸ“ˆ Estimated ${estimatedSecurities} securities â†’ ${totalChunks} chunks`
    );

    const chunks = Array.from({ length: totalChunks }, (_, i) => i);
    const results = [];

    // ë°°ì¹˜ë³„ë¡œ ë³‘ë ¬ ì‹¤í–‰ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
    const BATCH_SIZE = Math.min(MAX_PARALLEL, 1); // ì•ˆì •ì„±ì„ ìœ„í•´ 1ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batch = chunks.slice(i, i + BATCH_SIZE);
      console.log(
        `\nğŸ”„ Processing batch: chunks ${batch.map((c) => c + 1).join(", ")}`
      );

      const batchPromises = batch.map((chunkIndex) =>
        runBuildChunk(chunkIndex, totalChunks)
      );

      try {
        const batchResults = await Promise.all(batchPromises);
        results.push(...batchResults);
        console.log(
          `âœ… Batch completed: ${batchResults.length} chunks finished`
        );
      } catch (error) {
        console.error(`âŒ Batch failed:`, error.message);
        throw error;
      }
    }

    console.log(`\nğŸ‰ All ${totalChunks} chunks completed successfully!`);

    // ê²°ê³¼ ë³‘í•©
    await mergeResults(results);
  } catch (error) {
    console.error("âŒ Parallel build failed:", error.message);
    process.exit(1);
  }
}

/**
 * ê° ì²­í¬ì˜ ë¹Œë“œ ê²°ê³¼ë¥¼ ë³‘í•©
 */
async function mergeResults(results) {
  console.log("ğŸ”— Merging build results...");

  const fs = require("fs");
  const { execSync } = require("child_process");

  // ë©”ì¸ out ë””ë ‰í† ë¦¬ ì •ë¦¬
  if (fs.existsSync("out")) {
    execSync("rm -rf out");
  }
  fs.mkdirSync("out", { recursive: true });

  // ê° ì²­í¬ì˜ out ë””ë ‰í† ë¦¬ë¥¼ ë³‘í•©
  for (const result of results) {
    const chunkOutDir = `out-${result.chunkId}`;
    if (fs.existsSync(chunkOutDir)) {
      console.log(`ğŸ“ Merging ${chunkOutDir} into out/`);
      execSync(`cp -r ${chunkOutDir}/* out/`);

      // ì²­í¬ ë””ë ‰í† ë¦¬ ì •ë¦¬
      execSync(`rm -rf ${chunkOutDir}`);
    }
  }

  // ë¹Œë“œ ìºì‹œ ë””ë ‰í† ë¦¬ë“¤ë„ ì •ë¦¬
  for (const result of results) {
    const chunkDirs = [
      `.next-${result.chunkId}`,
      `.next-cache-${result.chunkId}`,
    ];

    chunkDirs.forEach((dir) => {
      if (fs.existsSync(dir)) {
        console.log(`ğŸ§¹ Cleaning up ${dir}`);
        execSync(`rm -rf ${dir}`);
      }
    });
  }

  console.log("âœ… Results merged successfully!");

  // ì‚¬ì´íŠ¸ë§µ ì¬ìƒì„±
  console.log("ğŸ—ºï¸ Regenerating sitemap...");
  const sitemapProcess = spawn("node", ["scripts/generate-sitemap.js"], {
    stdio: "inherit",
  });

  return new Promise((resolve, reject) => {
    sitemapProcess.on("close", (code) => {
      if (code === 0) {
        console.log("âœ… Sitemap regenerated!");
        resolve();
      } else {
        reject(new Error("Sitemap generation failed"));
      }
    });
  });
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if (require.main === module) {
  runParallelBuild();
}

module.exports = { runParallelBuild, runBuildChunk };
